{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOPWEX1QU1H2gBXLiVWjs6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bustion11/NN-projects/blob/main/ConvNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZdDK0r_19Wq"
      },
      "outputs": [],
      "source": [
        "#https://arxiv.org/pdf/2201.03545v2.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(nn.Module):\n",
        "  \"\"\"\n",
        "    Given features of shape (B, N, H, W).\n",
        "    The forward method randomly zeros out a whole feature set along N.\n",
        "\n",
        "    Args:\n",
        "      keep_p (float): how much features is left.\n",
        "      inplace (bool): whether it is inplace operation or not. \n",
        "  \"\"\"\n",
        "  def __init__(self, keep_p: float = 1.0, inplace: bool = False) -> None:\n",
        "    super().__init__()\n",
        "    self.keep_p = keep_p\n",
        "    self.inplace = inplace\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    if self.training:\n",
        "      mask_shape = (x.shape[0],) + (1,) * (x.ndim-1)\n",
        "      mask = x.new_empty(mask_shape).bernoulli(self.keep_p)\n",
        "\n",
        "      mask.div(self.keep_p)\n",
        "\n",
        "      return x.mul(mask) if self.inplace else (x * mask)\n",
        "      \n",
        "    return x\n",
        "\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "  \"\"\"\n",
        "    Classic layer normalization, but for images :)\n",
        "\n",
        "    Args:\n",
        "      dim (int): number of features/channels in the input.\n",
        "      epsilon (float): small value to handle the division by zero cases.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim: int, epsilon: float=1e-6) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.gamma = nn.Parameter(torch.ones(dim))\n",
        "    self.bias = nn.Parameter(torch.zeros(dim))\n",
        "    self.eps = epsilon\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    u = x.mean(1, keepdim=True)\n",
        "    s = (x-u).pow(2).mean(1, keepdim=True)\n",
        "    x = (x-u) / torch.sqrt(s+self.eps)\n",
        "    \n",
        "    return x * self.gamma[None, :, None, None] + self.bias[None, :, None, None]"
      ],
      "metadata": {
        "id": "Ykskq9epmsN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetblock(nn.Module):\n",
        "  \"\"\"\n",
        "    A whole block for the ConvNext architecture.\n",
        "\n",
        "    Args:\n",
        "      channels (int): number of features/channels in.\n",
        "      keep_prob (float): how much features/channels is left.\n",
        "      init_scale (float): learnable parameter. Initial scale of the output.\n",
        "  \"\"\"\n",
        "  def __init__(self, channels: int, keep_prob: float=1.0, init_scale: float=1e-6) -> None:\n",
        "    super().__init__()\n",
        "    self.dconv = nn.Conv2d(channels, channels, 7, padding=3, groups=channels)\n",
        "    self.conv1 = nn.Conv2d(channels, channels*4, 1)\n",
        "    self.conv2 = nn.Conv2d(channels*4, channels, 1)\n",
        "\n",
        "    self.ln = LayerNorm2d(channels)\n",
        "    self.act = nn.GELU()\n",
        "    self.drop = DropPath(keep_prob) if keep_prob != 1.0 else nn.Identity()\n",
        "    self.scale = (nn.Parameter(init_scale*torch.ones(channels), requires_grad=True)\n",
        "                  if init_scale > 0 else None) \n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    out = self.dconv(x)\n",
        "    out = self.ln(out)\n",
        "    out = self.act(self.conv1(out))\n",
        "    out = self.conv2(out)\n",
        "    if self.scale is not None:\n",
        "      out *= self.scale[None, :, None, None]\n",
        "    return x + self.drop(out)\n"
      ],
      "metadata": {
        "id": "uUOoiaj09b7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configurations as in the paper\n",
        "config_t = [(96, 192, 384, 768), (3, 3, 9, 3)] \n",
        "config_s = [(96, 192, 384, 768), (3, 3, 27, 3)]\n",
        "config_b = [(128, 256, 512, 1024), (3, 3, 27, 3)] \n",
        "config_l = [(192, 384, 768, 1536), (3, 3, 27, 3)] \n",
        "config_xl = [(256, 512, 1024, 2048), (3, 3, 27, 3)]\n"
      ],
      "metadata": {
        "id": "bEliKIj9K0C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetModel(nn.Module):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "      img_channels - number of channels in a single image.\n",
        "      configuration - configuration of the neural network. Possible configurations\n",
        "                      are included in the above cell.\n",
        "      n_classes - number of classes for classification task.\n",
        "    \"\"\"\n",
        "  def __init__(self, img_channels: int, configuration, n_classes: int) -> None:\n",
        "    super().__init__()\n",
        "    channels, blocks = configuration\n",
        "    self.down, self.blocks = nn.ModuleList([]), nn.ModuleList([])\n",
        "    dimensions, repeats = configuration\n",
        "\n",
        "    # \"stem\"\n",
        "    self.down.append(nn.Sequential(\n",
        "        nn.Conv2d(img_channels, dimensions[0], 4, 4),\n",
        "        LayerNorm2d(dimensions[0]),\n",
        "    ))\n",
        "    # downsample layers\n",
        "    for i in range(len(dimensions)-1):\n",
        "      self.down.append(nn.Sequential(\n",
        "          LayerNorm2d(dimensions[i]),\n",
        "          nn.Conv2d(dimensions[i], dimensions[i+1], 2, 2)\n",
        "      ))\n",
        "    \n",
        "    for i, repeat in enumerate(repeats):\n",
        "      block = []\n",
        "      for _ in range(repeat):\n",
        "        block.append(ConvNextblock(dimensions[i], 0.5))\n",
        "      self.blocks.append(nn.Sequential(\n",
        "          *block\n",
        "      ))\n",
        "    \n",
        "    self.norm = nn.LayerNorm(dimensions[-1])\n",
        "    self.classificator = nn.Linear(dimensions[-1], n_classes)\n",
        "  \n",
        "  def forward_extractor(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    for down, block in zip(self.down, self.blocks):\n",
        "      x = down(x)\n",
        "      x = block(x)\n",
        "    return self.norm(x.mean([-2, -1]))\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.classificator(self.forward_extractor(x))\n",
        "  "
      ],
      "metadata": {
        "id": "wweyuOA4yLx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNextModel(1, config_s, 10).to(DEVICE) # Initialize the model"
      ],
      "metadata": {
        "id": "Y-qH2fhPE37o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Dataset\n",
        "transforms = T.Compose([\n",
        "                        T.ToTensor(),\n",
        "                        T.Resize(64),\n",
        "                        T.RandomErasing(0.25)\n",
        "])\n",
        "\n",
        "train_dataset = FashionMNIST(\"/data\", train=True, transform=transforms, download=True)\n",
        "test_dataset = FashionMNIST(\"/data\", train=False, transform=transforms, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-8)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 30)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Anrf6z8vF0Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required loops\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.to(DEVICE)\n",
        "      y = y.to(DEVICE)\n",
        "\n",
        "      model.eval()\n",
        "      pred = model(X)\n",
        "      model.train()\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "EKNPUDERHByI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(3):\n",
        "  print(\"Epoch: \", _+1)\n",
        "  train_loop(train_loader, model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, model, loss_fn)\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ic-WXhGfLl",
        "outputId": "09887e49-ba6e-4298-9258-e9533c1cc17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n",
            "loss: 0.388046  [    0/60000]\n",
            "loss: 0.420315  [25600/60000]\n",
            "loss: 0.497963  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.461527 \n",
            "\n",
            "Epoch:  2\n",
            "loss: 0.443191  [    0/60000]\n",
            "loss: 0.398678  [25600/60000]\n",
            "loss: 0.546857  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.436707 \n",
            "\n",
            "Epoch:  3\n",
            "loss: 0.393759  [    0/60000]\n",
            "loss: 0.350044  [25600/60000]\n",
            "loss: 0.423006  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.421170 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
