{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPGol092Jlt3BVESOcknsLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bustion11/NN-projects/blob/main/SGAN/StyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31pOboB69Jkl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper classes\n",
        "class WSConv2d(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolution with equalised learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.scale = torch.tensor((gain/(in_channels * kernel_size ** 2)) ** 0.5)\n",
        "    self.bias = self.conv.bias\n",
        "    self.conv.bias = None\n",
        "\n",
        "      # initiliaze the layer\n",
        "\n",
        "    nn.init.normal_(self.conv.weight)\n",
        "    nn.init.zeros_(self.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x*self.scale)\n",
        "    x += self.bias.view(1, self.bias.shape[0], 1, 1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-8\n",
        "  def forward(self, x):\n",
        "    return x/torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, use_pn=True):\n",
        "    super().__init__()\n",
        "    self.conv = WSConv2d(in_channels, out_channels)\n",
        "    self.act = nn.LeakyReLU(0.2)\n",
        "    self.norm = PixelNorm() if use_pn else nn.Identity()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.norm(self.act(self.conv(x)))\n",
        "\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "  \"\"\"\n",
        "  Description: Adaptive instance normalization per each features channel.\n",
        "\n",
        "  Formula: \n",
        "          AdaIN(x, y) = y_style*((x_i-mean(x_i))/std(x_i))+y_bias\n",
        "\n",
        "  Input: x, style\n",
        "         x shape: (N, C, H, W);\n",
        "         style_s shape: (N, C);\n",
        "         style_b shape: (N, C);\n",
        "  Output: normalized x (N, C, H, W)\n",
        "  \"\"\"\n",
        "  def __init__(self, eps=1e-6):\n",
        "    self.eps = eps\n",
        "    super().__init__()\n",
        "  \n",
        "  def forward(self, x, y_s, y_b):\n",
        "    x = (x - torch.mean(input=x, dim=[-1, -2], keepdim=True)\n",
        "         /(torch.std(x, dim=[-1, -2], keepdim=True) + self.eps))\n",
        "    x = y_s[:, :, None, None] * x                                               #torch.einsum(\"n c h w, n c -> n c h w\", style_s, x)\n",
        "    return torch.add(x, y_b[:, :, None, None])\n",
        "\n",
        "\n",
        "class Adaptive_style(nn.Module):\n",
        "  \"\"\"\n",
        "  In the original paper: A\n",
        "\n",
        "  Desc: 'Learned affine transformations'\n",
        "  Input: W of shape (N, F)\n",
        "  Output: STYLE of shape (N, C, 1), (N, C, 1)\n",
        "  \"\"\"\n",
        "  def __init__(self, n_channels, features=512):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.y_s = nn.Linear(features, n_channels)\n",
        "    self.y_b = nn.Linear(features, n_channels)\n",
        "\n",
        "    nn.init.normal_(self.y_s.weight)\n",
        "    nn.init.normal_(self.y_b.weight)\n",
        "\n",
        "    nn.init.ones_(self.y_s.bias)\n",
        "    nn.init.zeros_(self.y_b.bias)\n",
        "\n",
        "  def forward(self, w):\n",
        "    y_style = self.y_s(w)\n",
        "    y_bias = self.y_b(w)\n",
        "\n",
        "    return y_style, y_bias\n",
        "\n",
        "\n",
        "class Adaptive_noise(nn.Module):\n",
        "  \"\"\"\n",
        "  In the original paper: B\n",
        "\n",
        "  Desc: 'Applies Learned per-channel scaling factors to the noise input'\n",
        "  Input: noise (N, 1, H, W)\n",
        "  Output: per_channel_scaled_noise (N, C, H, W)\n",
        "  \"\"\"\n",
        "  def __init__(self, n_channels):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.zeros(n_channels), requires_grad=True)\n",
        "\n",
        "  def forward(self, noise):\n",
        "    return noise*self.scale[None, :, None, None]"
      ],
      "metadata": {
        "id": "kKrH1yA0W0W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main block\n",
        "class Network_block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, features=512):\n",
        "    super().__init__()\n",
        "    self.convolution = ConvBlock(in_channels, out_channels)\n",
        "    self.B = Adaptive_noise(out_channels)\n",
        "    self.A = Adaptive_style(out_channels, features)\n",
        "    self.adain = AdaIN()\n",
        "  \n",
        "  def forward(self, x, w, noise):\n",
        "    x = self.convolution(x)\n",
        "    x = x + self.B(noise)\n",
        "    y_s, y_b = self.A(w)\n",
        "    return self.adain(x, y_s, y_b)"
      ],
      "metadata": {
        "id": "AAYXAy6GVwmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Network classes\n",
        "class Mapping_network(nn.Module):\n",
        "  \"\"\"\n",
        "  MLP network that maps latent noise Z to W\n",
        "\n",
        "  RELU IMPLEMENTED\n",
        "  \"\"\"\n",
        "  def __init__(self, n_features=512, normalization=None, n_layers=8):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.ModuleList([])\n",
        "    for _ in range(n_layers):\n",
        "      self.mlp.append(nn.Sequential(\n",
        "          nn.Linear(n_features, n_features),\n",
        "          nn.ReLU(),\n",
        "          normalization if normalization is not None else nn.Identity(),\n",
        "      ))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.mlp:\n",
        "      x = layer(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "config = [(512, 512), #8\n",
        "          (512, 512), #16\n",
        "          (512, 512), #32\n",
        "          (512, 256), #64\n",
        "          (256, 128), #128\n",
        "          (128, 64)]  #256\n",
        "\n",
        "class Synthesis_network(nn.Module):\n",
        "  \"\"\"\n",
        "  This is the main network. It implements such feature as progressive growing.\n",
        "  Forward pass arguments:\n",
        "    W, steps, alpha, batch_size\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, img_channels, channels=512, configuration=config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.const_noise = torch.ones(1, channels, 4, 4)\n",
        "\n",
        "    self.initial_block = nn.ModuleList([\n",
        "                                        Adaptive_noise(channels), # B\n",
        "                                        Adaptive_style(channels), # A\n",
        "                                        AdaIN(),\n",
        "                                        Network_block(channels, channels)\n",
        "    ])\n",
        "\n",
        "    self.initial_rgb = WSConv2d(channels, img_channels, 1, padding=0)\n",
        "\n",
        "    self.prog_blocks = nn.ModuleList([])\n",
        "    self.to_rgb = nn.ModuleList([])\n",
        "\n",
        "    for in_channels, out_channels in configuration: \n",
        "      self.prog_blocks.append(\n",
        "          nn.ModuleList([\n",
        "                         Network_block(in_channels, out_channels),\n",
        "                         Network_block(out_channels, out_channels)\n",
        "                         ]\n",
        "                        )\n",
        "          )\n",
        "      self.to_rgb.append(WSConv2d(out_channels, img_channels, 1, padding=0))\n",
        "\n",
        "  def _fade_in(self, alpha, upscaled, generated):\n",
        "    return torch.sigmoid(alpha*generated + (1.0-alpha)*upscaled)#torch.tanh(alpha*generated + (1.0-alpha)*upscaled)\n",
        "    \n",
        "  def _initial_pass(self, W, output, batch_size):\n",
        "    B, A, adain, net_block = self.initial_block\n",
        "    output += B(torch.randn(batch_size, 1, output.shape[2], output.shape[3])\n",
        "                .to(DEVICE))\n",
        "    y_s, y_b = A(W)\n",
        "    output = adain(output, y_s, y_b)\n",
        "    output = net_block(output, W,\n",
        "                       (torch.randn(batch_size, 1, output.shape[2], output.shape[3])\n",
        "                       .to(DEVICE)))\n",
        "    return output\n",
        "\n",
        "  def forward(self, W, steps, alpha, batch_size):\n",
        "    # assert steps > len(self.prog_blocks)+1\n",
        "    # Batchify the constant noise\n",
        "    output = torch.repeat_interleave(self.const_noise, batch_size, 0).to(DEVICE)\n",
        "\n",
        "    # Initial block\n",
        "    # -------------\n",
        "    output = self._initial_pass(W, output, batch_size)\n",
        "    # -------------\n",
        "    \n",
        "    # Main block\n",
        "    # -------------\n",
        "    if steps == 0:\n",
        "      return torch.sigmoid(self.initial_rgb(output)) #torch.tanh(self.initial_rgb(output))\n",
        "    \n",
        "    for step in range(steps):\n",
        "      upscaled = torch.nn.functional.interpolate(output, scale_factor=2, mode=\"bilinear\") \n",
        "      layer1, layer2 = self.prog_blocks[step]\n",
        "      upscaled = layer1(upscaled, W, \n",
        "                        (torch.randn(batch_size, 1, upscaled.shape[2], upscaled.shape[3])\n",
        "                        .to(DEVICE)))\n",
        "      output = layer2(upscaled, W, \n",
        "                      (torch.randn(batch_size, 1, upscaled.shape[2], upscaled.shape[3])\n",
        "                      .to(DEVICE)))\n",
        "\n",
        "    rgb_upscaled = self.to_rgb[steps-1](upscaled)\n",
        "    rgb_output = self.to_rgb[steps-1](output)\n",
        "    # -------------\n",
        "    \n",
        "    return self._fade_in(alpha, rgb_upscaled, rgb_output) \n",
        "\n",
        "  def manual_forward(self, x, W, step, alpha, batch_size, to_rgb=False):\n",
        "    output = x\n",
        "    \n",
        "    if step == 0:\n",
        "      output = self._initial_pass(W, output, batch_size)\n",
        "      return self.initial_rgb(output) if to_rgb else output\n",
        "    \n",
        "    upscaled = torch.nn.functional.interpolate(output, scale_factor=2, mode=\"bilinear\")\n",
        "    layer1, layer2 = self.prog_blocks[step]\n",
        "    upscaled = layer1(upscaled, W, \n",
        "                        (torch.randn(batch_size, 1, upscaled.shape[2], upscaled.shape[3])\n",
        "                        .to(DEVICE)))\n",
        "    output = layer2(upscaled, W, \n",
        "                      (torch.randn(batch_size, 1, upscaled.shape[2], upscaled.shape[3])\n",
        "                      .to(DEVICE)))\n",
        "    \n",
        "    rgb_upscaled = self.to_rgb[step](upscaled) if to_rgb else upscaled\n",
        "    rgb_output = self.to_rgb[step](output) if to_rgb else output\n",
        "\n",
        "    return self._fade_in(alpha, rgb_upscaled, rgb_output)"
      ],
      "metadata": {
        "id": "kw8Rk7GebMsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, img_channels, channels=512, configuration=config):\n",
        "    super().__init__()\n",
        "    self.dim = channels\n",
        "\n",
        "    configuration = configuration[::-1]\n",
        "    self.blocks, self.from_rgb = nn.ModuleList([]), nn.ModuleList([])\n",
        "    self.leaky = nn.LeakyReLU(0.2)\n",
        "    self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "    for channels in configuration:\n",
        "      in_channels, out_channels = [*reversed(channels)]\n",
        "      self.blocks.append(nn.Sequential(\n",
        "          WSConv2d(in_channels, out_channels),\n",
        "          self.leaky,\n",
        "          WSConv2d(out_channels, out_channels),\n",
        "          self.leaky\n",
        "      ))\n",
        "      \n",
        "      self.from_rgb.append(WSConv2d(img_channels, in_channels, 1, 1, 0))\n",
        "    \n",
        "    final_rgb = WSConv2d(img_channels, self.dim, 1, 1, 0)\n",
        "    self.final_block = nn.Sequential(\n",
        "        WSConv2d(self.dim+1, self.dim, 3, 1, 1),\n",
        "        self.leaky,\n",
        "        WSConv2d(self.dim, self.dim, 4, 1, 0),\n",
        "        self.leaky,\n",
        "        WSConv2d(self.dim, 1, 1, 1, 0)\n",
        "    )\n",
        "\n",
        "    self.from_rgb.append(final_rgb)\n",
        "\n",
        "  def _fade_in(self, alpha, downscaled, out):\n",
        "    return alpha*out + (1-alpha)*downscaled\n",
        "  \n",
        "  def minibatch_std(self, x):\n",
        "    batch_statistics = (torch.std(x, dim=0).mean()\n",
        "    .repeat(x.shape[0], 1, x.shape[2], x.shape[3]))\n",
        "    \n",
        "    return torch.cat([x, batch_statistics], dim=1)\n",
        "  \n",
        "  def forward(self, x, steps, alpha):\n",
        "    cur_step = len(self.blocks) - steps\n",
        "    out = self.leaky(self.from_rgb[cur_step](x))\n",
        "\n",
        "    if steps == 0:\n",
        "      print(\"ERROR\")\n",
        "      out = self.minibatch_std(out)\n",
        "      return self.final_block(out).reshape(out.shape[0], -1) \n",
        "\n",
        "    downscaled = self.leaky(self.from_rgb[cur_step +  1](self.pool(x)))\n",
        "    out = self.pool(self.blocks[cur_step](out))\n",
        "    out = self._fade_in(alpha, downscaled, out)\n",
        "\n",
        "    for step in range(cur_step+1, len(self.blocks)):\n",
        "      out = self.blocks[step](out)\n",
        "      out = self.pool(out)\n",
        "\n",
        "    out = self.minibatch_std(out)\n",
        "    return self.final_block(out).reshape(out.shape[0], -1)\n"
      ],
      "metadata": {
        "id": "3Ec5hZ4hP_rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_CHANNELS=1"
      ],
      "metadata": {
        "id": "QbLaLifMgwcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn(5, 512).to(DEVICE)"
      ],
      "metadata": {
        "id": "Q_8ZOQK9cgnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Synthesis_network(IMG_CHANNELS)\n",
        "critic = Discriminator(IMG_CHANNELS)"
      ],
      "metadata": {
        "id": "64qRlCMEc3lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step=4\n",
        "alpha=1\n",
        "batch_size=5"
      ],
      "metadata": {
        "id": "eDLKgFyrc0E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = generator(W, step, alpha, batch_size)\n",
        "scores = critic(generated_image, step, alpha)"
      ],
      "metadata": {
        "id": "Q-I5oYWTdA_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image[4][0]"
      ],
      "metadata": {
        "id": "CtzLxaxNd8Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.reshape(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrbiBraudhUS",
        "outputId": "f55b0087-b0ac-4361-f8a9-702f726868f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0100, -0.0100,  0.0005,  0.0029, -0.0100],\n",
              "       grad_fn=<ReshapeAliasBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(steps, batch_size=8):\n",
        "  img_size = 4 * 2**steps\n",
        "  data = FashionMNIST(root=\"/data\", transform=T.Compose([\n",
        "                                                         T.Resize(img_size),\n",
        "                                                         T.ToTensor(),\n",
        "                                                         T.Normalize([0.5 for _ in range(IMG_CHANNELS)],\n",
        "                                                                     [0.5 for _ in range(IMG_CHANNELS)]),\n",
        "  ]), download=True)\n",
        "\n",
        "  dataloader = torch.utils.data.DataLoader(data, batch_size, True, \n",
        "                                           num_workers=os.cpu_count(),\n",
        "                                           pin_memory=True)\n",
        "  return dataloader, data"
      ],
      "metadata": {
        "id": "_yKaqiKptl34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_penalty(real, critic, step, alpha):\n",
        "  real = torch.autograd.Variable(real, requires_grad=True)\n",
        "  real_scores = critic(real, step, alpha)\n",
        "  penalty = torch.autograd.grad(\n",
        "        inputs=real,\n",
        "        outputs=real_scores,\n",
        "        grad_outputs=torch.ones_like(real_scores).to(DEVICE),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0].reshape(real.shape[0], -1)\n",
        "\n",
        "  penalty = torch.norm(penalty, dim=1).pow(2)\n",
        "  return penalty.mean()"
      ],
      "metadata": {
        "id": "T4L4tsDAWIse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For CPU\n",
        "def train_loop(alpha, step, loader, dataset,\n",
        "               generator, style_generator, critic,\n",
        "               opt_generator, opt_s_generator, opt_critic, loss_fn):\n",
        "  generator.train()\n",
        "  style_generator.train()\n",
        "  critic.train()\n",
        "  for batch_idx, (x, _) in enumerate(loader):\n",
        "    # Sample Z\n",
        "    x = x.to(DEVICE)\n",
        "    Z = torch.randn(x.shape[0], 512).to(DEVICE)\n",
        "\n",
        "    # Train critic\n",
        "    W = style_generator(Z)\n",
        "    fake = generator(W, step, alpha, x.shape[0])\n",
        "\n",
        "    real_scores = critic(x, step, alpha).reshape(-1)\n",
        "    real_loss = loss_fn(real_scores, torch.ones_like(real_scores))\n",
        "\n",
        "    fake_scores = critic(fake, step, alpha).reshape(-1)\n",
        "    fake_loss= loss_fn(fake_scores, torch.zeros_like(fake_scores))\n",
        "\n",
        "    penalty = calculate_penalty(x.detach(), critic, step, alpha)\n",
        "\n",
        "    loss_critic = (real_loss+fake_loss)/2 + 10/2 * penalty\n",
        "\n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Debug: \")\n",
        "      print(\"Alpha: \", alpha)\n",
        "      print(\"Critic loss: \", loss_critic.item())\n",
        "      print(\"Penalty: \", penalty.item())\n",
        "\n",
        "\n",
        "    opt_critic.zero_grad()\n",
        "    loss_critic.backward(retain_graph=True)\n",
        "    opt_critic.step()\n",
        "\n",
        "    # Train generator\n",
        "    \n",
        "    scores = critic(fake, step, alpha).reshape(-1)\n",
        "    loss_generator = loss_fn(scores, torch.ones_like(scores)) \n",
        "    \n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Generator loss:\", loss_generator.item())\n",
        "    \n",
        "    opt_generator.zero_grad()\n",
        "    loss_generator.backward(retain_graph=True)\n",
        "    opt_generator.step()\n",
        "\n",
        "    opt_s_generator.zero_grad()\n",
        "    opt_s_generator.step()\n",
        "\n",
        "    alpha += x.shape[0] / ((x.shape[0]*0.5)*len(dataset))\n",
        "    alpha = min(alpha, 1)\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "owbbPH-oaA6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Synthesis_network(IMG_CHANNELS).to(DEVICE)\n",
        "style_generator = Mapping_network().to(DEVICE)\n",
        "critic = Discriminator(IMG_CHANNELS).to(DEVICE)"
      ],
      "metadata": {
        "id": "hZe2rUz2xVZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_critic = torch.optim.Adam(critic.parameters(), lr=2e-3)\n",
        "opt_generator = torch.optim.Adam(generator.parameters(), lr=2e-3)\n",
        "opt_style_gen = torch.optim.Adam(style_generator.parameters(), lr=2e-3*0.01)\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Xw7uq4Nc20Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=1e-5"
      ],
      "metadata": {
        "id": "Xfyn44bJv2vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader, dataset = get_loader(2)\n",
        "for _ in range(5):\n",
        "  print(\"Step: \", _+1)\n",
        "  alpha = train_loop(alpha, 2, loader, dataset, generator, style_generator,\n",
        "                      critic, opt_generator, opt_style_gen, opt_critic, loss_fn)"
      ],
      "metadata": {
        "id": "9ceb2LWipuPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 1\n",
        "for _ in range(5):\n",
        "  \n",
        "  alpha = 1e-5\n",
        "  loader, dataset = get_loader(step)\n",
        "  print(f\"Current image size: {4 * 2 ** step}\")\n",
        "\n",
        "  for epoch in range(10):\n",
        "    print(f\"Epoch: [{epoch+1}/{10}]\")\n",
        "    alpha = train_loop(alpha, step, loader, dataset, generator, style_generator,\n",
        "                      critic, opt_generator, opt_style_gen, opt_critic, loss_fn)\n",
        "  step += 1"
      ],
      "metadata": {
        "id": "RGaW4Pp24SDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn(8, 512).to(DEVICE)"
      ],
      "metadata": {
        "id": "Ysk5Y78TM2Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_generator.eval()\n",
        "generator.eval()\n",
        "W = style_generator(noise)\n",
        "img = generator(noise, 1, 1, 8)\n",
        "plt.imshow(img.detach().cpu()[3][0])"
      ],
      "metadata": {
        "id": "7D-GjnV9M57-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For GPU \n",
        "# IT DOES NOT WORK\n",
        "def train_loop_gpu(alpha, step, loader, dataset,\n",
        "                   generator, style_generator, critic,\n",
        "                   opt_generator, opt_s_generator, opt_critic, loss_fn,\n",
        "                   gs_generator, gs_s_generator, gs_critic\n",
        "                   ):\n",
        "  for batch_idx, (x, _) in enumerate(loader):\n",
        "    # Sample Z\n",
        "    x = x.to(DEVICE)\n",
        "    Z = torch.randn(x.shape[0], 512).to(DEVICE)\n",
        "\n",
        "    # Train critic\n",
        "    with torch.cuda.amp.autocast():\n",
        "      W = style_generator(Z)\n",
        "      fake = generator(W, step, alpha, x.shape[0])\n",
        "      print(fake)\n",
        "      \n",
        "      real_scores = critic(x, step, alpha)#.reshape(-1)\n",
        "      real_loss = loss_fn(real_scores, torch.ones_like(real_scores))\n",
        "\n",
        "      fake_scores = critic(fake, step, alpha)#.reshape(-1)\n",
        "      fake_loss= loss_fn(fake_scores, torch.zeros_like(fake_scores)) \n",
        "\n",
        "      loss_critic = (real_loss+fake_loss)/2\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"Debug: \")\n",
        "      print(\"Alpha: \", alpha)\n",
        "      print(\"Critic loss:\", loss_critic.item())\n",
        "      #print(fake)\n",
        "\n",
        "\n",
        "    opt_critic.zero_grad()\n",
        "    gs_critic.scale(loss_critic).backward(retain_graph=True)\n",
        "    gs_critic.step(opt_critic)\n",
        "    gs_critic.update()\n",
        "\n",
        "\n",
        "    # Train generator\n",
        "    with torch.cuda.amp.autocast():\n",
        "      scores = critic(fake, step, alpha).reshape(-1)\n",
        "      loss_generator = loss_fn(scores, torch.ones_like(scores)) \n",
        "    \n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"Generator loss:\", loss_generator.item())\n",
        "    \n",
        "    opt_generator.zero_grad()\n",
        "    gs_generator.scale(loss_generator).backward(retain_graph=True)\n",
        "    gs_generator.step(opt_generator)\n",
        "    gs_generator.update()\n",
        "\n",
        "    opt_s_generator.zero_grad()\n",
        "    gs_s_generator.scale(loss_generator).backward()\n",
        "    gs_s_generator.step(opt_s_generator)\n",
        "    gs_s_generator.update()\n",
        "\n",
        "    alpha += x.shape[0] / ((x.shape[0]*0.5)*len(dataset))\n",
        "    alpha = min(alpha, 1)\n",
        "\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "BowUSn4gIhf6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}