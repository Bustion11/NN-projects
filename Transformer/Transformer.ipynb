{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH0yaw24KkpMo1k5eRn21P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bustion11/NN-projects/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nSHPNY23PHm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchtext.vocab as TV\n",
        "from torch.nn import functional as F\n",
        "import timeit\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterable = {\"I\":1, \"ate\":2, \"an\":3, \"apple\":4}"
      ],
      "metadata": {
        "id": "oe1CQhGLbHed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working sentence encoder w/ detection of new words\n",
        "class ModVocab(TV.Vocab):\n",
        "  def __init__(self, iterable):\n",
        "    vocab = TV.vocab(iterable)\n",
        "    super().__init__(vocab)\n",
        "  \n",
        "  def forward(self, tokens):\n",
        "    dictionary = super().get_itos()\n",
        "    for token in tokens:\n",
        "      if(not super().__contains__(token)):\n",
        "        super().append_token(token)\n",
        "    \n",
        "    return super().forward(tokens)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, emb_dim, n_heads = 1, k_dim = None, v_dim = None):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(emb_dim, n_heads, k_dim, v_dim)\n",
        "    self.norm = nn.LayerNorm(emb_dim)\n",
        "    self.lin = PointWiseFFN(emb_dim, emb_dim * 2)\n",
        "  \n",
        "  def forward(self, x, causal_mask = None):\n",
        "    out, mtx = self.mha(x, x, x, causal_mask)\n",
        "\n",
        "    x = self.norm(x + out)\n",
        "\n",
        "    out = self.lin(x, causal_mask)\n",
        "\n",
        "    x = self.norm(x + out)\n",
        "\n",
        "    return x, mtx\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, emb_dim, n_heads = 1, k_dim = None, v_dim = None):\n",
        "    super().__init__()\n",
        "    self.masked_mha = MultiHeadAttention(emb_dim, n_heads, k_dim, v_dim)\n",
        "    self.middle_mha = MultiHeadAttention(emb_dim, n_heads, k_dim, v_dim)\n",
        "    self.final_mha = MultiHeadAttention(emb_dim, n_heads, k_dim, v_dim)\n",
        "\n",
        "    self.project = PointWiseFFN(emb_dim, emb_dim*2)\n",
        "    self.norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, encoder_output, decoder_output, causal_mask = None, attn_mask = None):\n",
        "    out, mtx = self.masked_mha(decoder_output, decoder_output, decoder_output, causal_mask = causal_mask, attn_mask = attn_mask)\n",
        "    decoder_output = self.norm(out) + decoder_output\n",
        "\n",
        "    out, mtx = self.middle_mha(encoder_output, encoder_output, decoder_output, causal_mask = causal_mask)\n",
        "    decoder_output = self.norm(out) + decoder_output\n",
        "\n",
        "    out, mtx = self.final_mha(decoder_output, decoder_output, decoder_output, causal_mask = causal_mask)\n",
        "    decoder_output = self.norm(out) + decoder_output\n",
        "\n",
        "    out = self.project(decoder_output)\n",
        "    decoder_output = self.norm(out) + decoder_output\n",
        "\n",
        "    return decoder_output\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "  \n",
        "  def forward(self, q, k, v, attn_mask = None, causal_mask = None):\n",
        "    k = torch.transpose(k, 1, 2)\n",
        "    attn_mtx = torch.bmm(q, k)\n",
        "    attn_mtx = torch.div(attn_mtx, k.shape[0]**(1/2))\n",
        "  \n",
        "    if attn_mask is not None:\n",
        "      attn_mtx = attn_mtx + attn_mask\n",
        "\n",
        "    attn_mtx = F.softmax(attn_mtx, -1)\n",
        "\n",
        "    if causal_mask is not None:\n",
        "      attn_mtx = attn_mtx * causal_mask\n",
        "      \n",
        "    v = torch.bmm(attn_mtx, v)\n",
        "  \n",
        "    return v, attn_mtx\n",
        "    \n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, emb_dim, n_heads = 1, k_dim = None, v_dim = None):\n",
        "    super().__init__()\n",
        "\n",
        "    if k_dim is None:\n",
        "      k_dim = emb_dim\n",
        "    \n",
        "    if v_dim is None:\n",
        "      v_dim = emb_dim\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    head_dim = emb_dim//n_heads\n",
        "\n",
        "    self.q_proj = self._build_stack(nn.Linear(emb_dim, head_dim, False))\n",
        "    self.k_proj = self._build_stack(nn.Linear(k_dim, head_dim, False))\n",
        "    self.v_proj = self._build_stack(nn.Linear(v_dim, head_dim, False))\n",
        "\n",
        "    self.attn = Attention()\n",
        "\n",
        "    self.o_proj = nn.Linear(head_dim*n_heads, emb_dim, False)\n",
        "\n",
        "  def _build_stack(self, layer: nn.Module):\n",
        "    stack = nn.ModuleList()\n",
        "    for i in range(self.n_heads):\n",
        "      stack.append(layer)\n",
        "    return stack\n",
        "\n",
        "  def _apply(self, x: torch.Tensor, layer_stack: nn.ModuleList) -> torch.Tensor:\n",
        "    if self.n_heads == 1:\n",
        "      return layer_stack[0](x)\n",
        "\n",
        "    B, N, E = x.shape\n",
        "    x = x.unsqueeze(1)\n",
        "    temp = layer_stack[0](x)\n",
        "\n",
        "    for i in range(1, self.n_heads):\n",
        "      temp = torch.cat((temp, layer_stack[i](x)), 1)\n",
        "\n",
        "    temp = temp.reshape(B*self.n_heads, N, E//self.n_heads)\n",
        "    return temp\n",
        "  \n",
        "  def forward(self, q, k, v, causal_mask: torch.Tensor = None, attn_mask = None):\n",
        "    if causal_mask is not None:\n",
        "      causal_mask = causal_mask.repeat_interleave(self.n_heads, 0)\n",
        "\n",
        "    q = self._apply(q, self.q_proj)\n",
        "    k = self._apply(k, self.k_proj)\n",
        "    v = self._apply(v, self.v_proj)\n",
        "\n",
        "    output, attn_mtx = self.attn(q, k, v, attn_mask, causal_mask)\n",
        "\n",
        "    B, N, E = output.shape\n",
        "    output = output.transpose(0, 1).reshape(N, B//self.n_heads, E*self.n_heads).transpose(0, 1)\n",
        "    \n",
        "    output = self.o_proj(output)\n",
        "    return output, attn_mtx\n",
        "\n",
        "\n",
        "class PointWiseFFN(nn.Module):\n",
        "  def __init__(self, in_emb_dim, inner_dim, out_emb_dim = None):\n",
        "    super().__init__()\n",
        "    if out_emb_dim is None:\n",
        "      out_emb_dim = in_emb_dim\n",
        "    self.l1 = nn.Linear(in_emb_dim, inner_dim)\n",
        "    self.l2 = nn.Linear(inner_dim, out_emb_dim)\n",
        "    self.act = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, causal_mask = None):\n",
        "    assert len(x.shape) == 3\n",
        "    if causal_mask is None:\n",
        "      causal_mask = torch.tensor([1])\n",
        "\n",
        "    x = self.l2(self.act(self.l1(x))*causal_mask)*causal_mask\n",
        "\n",
        "    return x\n",
        "    \n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, num_words, sequence_len, emb_dim, pad_idx = 0):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(num_words, emb_dim, pad_idx)\n",
        "    self.seq_len = sequence_len\n",
        "    self.emb_dim = emb_dim\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "    self.register_buffer('PE', self.calculate_pos_emb(sequence_len, emb_dim))\n",
        "\n",
        "  def calculate_pos_emb(self, sentence_len, emb_dim):\n",
        "    power = torch.arange(0, emb_dim, 2).div(emb_dim)\n",
        "    pos = torch.arange(0, sentence_len, 1).unsqueeze(1).repeat_interleave(emb_dim//2, -1)\n",
        "    pos = pos.div(torch.tensor(10000).pow(power))\n",
        "    pe = torch.zeros(sentence_len, emb_dim)\n",
        "    pe[:, 0::2] = torch.sin(pos)\n",
        "    pe[:, 1::2] = torch.cos(pos)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    return pe\n",
        "\n",
        "  def forward(self, x):\n",
        "    padding_mask = torch.where(x == self.pad_idx, x, 1).unsqueeze(-1)\n",
        "    \n",
        "    x = self.embed(x)\n",
        "    x = x + self.get_buffer('PE')\n",
        "    return x*padding_mask, padding_mask"
      ],
      "metadata": {
        "id": "NyJ02rLDfw5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_words, seq_len, emb_dim, pad_idx = 0, shared_n_heads = 1, num_encoder = 1, num_decoder = 1):\n",
        "    super().__init__()\n",
        "    self.encoder_stack = nn.ModuleList()\n",
        "    self.decoder_stack = nn.ModuleList()\n",
        "\n",
        "    for i in range(num_encoder):\n",
        "      self.encoder_stack.append(Encoder(emb_dim, shared_n_heads))\n",
        "\n",
        "    for i in range(num_decoder):\n",
        "      self.decoder_stack.append(Decoder(emb_dim, shared_n_heads))\n",
        "\n",
        "    self.embeder_encoder = PositionalEmbedding(num_words, seq_len, emb_dim)\n",
        "    self.embeder_decoder = PositionalEmbedding(num_words, seq_len, emb_dim)\n",
        "\n",
        "    attn_mask = torch.full((1, seq_len, seq_len), -torch.inf).triu(1)\n",
        "    self.register_buffer('attn_mask', attn_mask)\n",
        "\n",
        "  def forward(self, input, output):\n",
        "    encoder_output, causal_mask_input = self.embeder_encoder(input)\n",
        "    decoder_output, causal_mask_output = self.embeder_decoder(output)\n",
        "\n",
        "    for encoder_layer in self.encoder_stack:\n",
        "      encoder_output, encoder_mtx = encoder_layer(encoder_output, causal_mask_input)\n",
        "    \n",
        "    for decoder_layer in self.decoder_stack:\n",
        "      decoder_output = decoder_layer(encoder_output, decoder_output, causal_mask_output, self.get_buffer('attn_mask'))\n",
        "    \n",
        "    return decoder_output\n"
      ],
      "metadata": {
        "id": "Mw312hLNoipL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 100\n",
        "NUM_CLASS = 10\n",
        "SEQ_LEN = 64\n",
        "EMB_DIM = 128\n",
        "N_HEADS = 8\n",
        "\n",
        "test_input = torch.randint(1, 100, (3, 64))"
      ],
      "metadata": {
        "id": "6aWN40nZeAkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDc9pnLdfEpq",
        "outputId": "a2b6875a-eb08-4bcf-ca78-75e1c673aac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[98, 78, 61, 85, 31, 32, 29, 69, 99,  6, 26, 56, 48, 43, 54, 26, 16, 48,\n",
            "         85, 37, 73, 33, 52,  6,  6, 79, 87, 35, 33, 29, 56, 79, 78, 68, 59, 48,\n",
            "         57, 94, 62, 29, 49, 68, 74, 12, 22, 32, 55,  2, 99, 90, 95, 73, 41, 11,\n",
            "         23, 89, 57, 54, 48, 77, 44, 48, 54,  1],\n",
            "        [27, 88, 34, 31, 63, 49, 13, 92, 11, 36, 74,  7, 37, 31, 29, 31, 81,  5,\n",
            "         95, 12,  2, 99, 37, 97,  1, 53, 29, 60, 37, 16, 63, 74, 77, 46, 72, 48,\n",
            "         63, 37, 80, 70, 67,  2, 34, 91, 64, 14, 64, 81, 52, 21, 54, 72, 55, 98,\n",
            "         46, 31, 80, 46, 14, 18, 50, 80, 19, 94],\n",
            "        [43, 27, 49, 77, 80, 88, 16, 72,  8, 65, 55, 36, 42, 90, 35, 26, 68, 51,\n",
            "         93, 69, 79, 32, 80, 77, 73, 46, 32, 82, 21, 82, 57, 60, 99, 68, 96, 53,\n",
            "         49, 86, 58, 16, 32, 30, 78, 41, 17, 48, 90,  5, 66, 79, 77, 97,  3, 42,\n",
            "         49, 34, 52, 40, 79, 63, 97, 46,  9, 46]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = Transformer(NUM_WORDS, NUM_CLASS, SEQ_LEN, EMB_DIM, 0, N_HEADS, 4, 4)"
      ],
      "metadata": {
        "id": "HrRNkAaJfNcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_output = test_model(test_input, test_input)"
      ],
      "metadata": {
        "id": "ICsk8qAbghfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(-1)"
      ],
      "metadata": {
        "id": "NlQI0hk2jHSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(softmax(test_output)*100).round()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDVv2k4PmaUq",
        "outputId": "e7038973-fe35-4449-f4ab-18616fee95b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  0.,  2., 56.,  0.,  5.,  2., 17., 16.,  1.],\n",
              "        [ 0.,  0.,  1., 67.,  0.,  1.,  1., 11., 17.,  1.],\n",
              "        [ 0.,  0.,  1., 73.,  0.,  7.,  1.,  5., 12.,  0.]],\n",
              "       grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "-sBgICkbR_fo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
