{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrIsRciW4cfcRpV4lCpUey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bustion11/NN-projects/blob/main/UNet/U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcg6-tpDirSG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "  def _block(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(out_channel, out_channel, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "  def __init__(self, img_channels, n_classes, features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.img_channels = img_channels\n",
        "    self.n_classes = n_classes\n",
        "\n",
        "    self.down = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    self.stack_down, self.stack_up, self.stack_upscale = nn.ModuleList([]), nn.ModuleList([]), nn.ModuleList([])\n",
        "\n",
        "    for feature in features:\n",
        "      self.stack_down.append(self._block(img_channels, feature))\n",
        "      img_channels = feature\n",
        "\n",
        "    for feature in reversed(features):\n",
        "      self.stack_up.append(self._block(feature*2, feature))\n",
        "      self.stack_upscale.append(nn.ConvTranspose2d(feature*2, feature, 2, 2))\n",
        "    \n",
        "    self.center = self._block(features[-1], features[-1]*2)\n",
        "    \n",
        "    self.final = nn.Conv2d(features[0], n_classes, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    samples = []\n",
        "    for layer in self.stack_down:\n",
        "      x = layer(x)\n",
        "      samples.append(x)\n",
        "      x = self.down(x)\n",
        "    \n",
        "    x = self.center(x)\n",
        "\n",
        "    for (up, up_scale, sample) in zip(self.stack_up, self.stack_upscale, reversed(samples)):\n",
        "      x = up_scale(x)\n",
        "      if x.shape != sample.shape:\n",
        "        x = transforms.functional.resize(x, sample.shape[2:])\n",
        "      x = torch.cat((x, sample), dim=1)\n",
        "      x = up(x)\n",
        "\n",
        "    return self.final(x)"
      ],
      "metadata": {
        "id": "cHyO1TLRsGcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(3, 2)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "_1CL9hm6z5uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-4\n",
        "IMG_CHANNELS = 3\n",
        "N_CLASSES = None #To be assigned\n",
        "TARGET_TYPE = 'semantic'\n",
        "N_WORKERS = os.cpu_count()\n",
        "IMAGE_HEIGHT = 200\n",
        "IMAGE_WIDTH = 200"
      ],
      "metadata": {
        "id": "j8A0ygB-MBez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cityscapes('/data/cityscapes', \"train\", target_type=TARGET_TYPE)\n",
        "#Cityscapes('/data/cistyscapes', \"test\", target_type=TARGET_TYPE)"
      ],
      "metadata": {
        "id": "ZrE0ne-e9DbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "xmQUmMMrAatd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(BATCH_SIZE, N_WORKERS, IMG_TRANSFORM, TARGET_TRANSFORM, DATASET:Dataset):\n",
        "  train_loader = None\n",
        "  test_loader = None\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "--O8F5m769xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, optimizer, scaler, train_loader, test_loader, loss_fn=nn.BCEWithLogitsLoss()):\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    with torch.cuda.amp.autocast():\n",
        "      loss = loss_fn(model(x), y)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.stap(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    if batch_idx%100==0:\n",
        "      print(loss.item)"
      ],
      "metadata": {
        "id": "OttVUl80-7aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([\n",
        "                             A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "                             A.Rotate(limit=35, p=1.0),\n",
        "                             A.HorizontalFlip(p=0.5),\n",
        "                             A.VerticalFlip(p=0.1),\n",
        "                             A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
        "                             #ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "                            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "                            A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
        "                            #ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "1lfJD2jAAKV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}